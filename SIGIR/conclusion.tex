\section{Conclusion}
\label{sec:conclusion}
Word embeddings have become a popular means to represent textual information in low-dimensional space. 
Several models have been proposed recently that exploit these embeddings to train neural architectures 
that perform significantly better than traditional models that relied on manually coded features. 
We argue that models based on word embeddings ignore a key aspect of web documents i.e. their structure. 
Each word in a web page can also be represented using its structural meta-data such as its location, size or 
style in webpage. Existing deep models do not encode structural information in a webpage in any form which 
traditionally has been used extensively to design query or document specific features. 

In this work, we tried to address the above shortcoming of existing models by proposing a simple 
neural framework that also encodes structural information of web page text to perform query-document matching. 
Our network takes query and document vectors (along with structure information) as input to predict document 
relevance. Preliminary experiments show that our approach performs better than existing learning-to-rank models 
trained on hand crafted features. We posit that our model automatically learns important textual 
and structural features to score a document against a search query. In future it would be worth 
investigating whether structural embeddings would be useful in tasks such 
as webpage clustering 
or classification.
